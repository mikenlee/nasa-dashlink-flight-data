{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the Output, check that the alignment issue is fixed:\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Load a processed file\n",
    "df = pl.read_parquet('../data/processed/ml_ready/tail_number=672/flight_001.parquet')\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "\n",
    "# Check for NAs in ALT - should be minimal now\n",
    "alt_na_pct = df['ALT'].null_count() / len(df) * 100\n",
    "print(f\"ALT NA %: {alt_na_pct:.2f}%\")\n",
    "\n",
    "# Verify time alignment\n",
    "print(f\"Duration: {df['time_sec'].max() / 60:.1f} minutes\")\n",
    "print(f\"Sample rate: {len(df) / df['time_sec'].max():.1f} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b4c1a",
   "metadata": {},
   "source": [
    "# Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_dir': Path('../data/processed/ml_ready'),\n",
    "    'window_size': 256,      # 64 seconds at 4 Hz\n",
    "    'stride': 64,            # 16 seconds stride (75% overlap)\n",
    "    'batch_size': 64,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'latent_dim': 64,\n",
    "    'n_heads': 4,\n",
    "    'n_layers': 2,\n",
    "    'dropout': 0.1,\n",
    "    'train_split': 0.8,\n",
    "}\n",
    "\n",
    "# Engine + context signals\n",
    "ENGINE_SIGNALS = ['N1_1', 'N1_2', 'N2_1', 'N2_2', 'EGT_1', 'EGT_2', 'FF_1', 'FF_2']\n",
    "CONTEXT_SIGNALS = ['ALT', 'MACH', 'PTCH']\n",
    "ALL_SIGNALS = ENGINE_SIGNALS + CONTEXT_SIGNALS\n",
    "\n",
    "print(f\"Signals: {len(ALL_SIGNALS)}\")\n",
    "print(f\"Window: {CONFIG['window_size']} samples ({CONFIG['window_size']/4:.0f} sec)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc6d87",
   "metadata": {},
   "source": [
    "\n",
    "# 2: Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flight_data(data_dir: Path, signals: list, max_flights: int = None) -> list:\n",
    "    \"\"\"Load flight data from processed parquet files.\"\"\"\n",
    "    flights = []\n",
    "    parquet_files = sorted(data_dir.glob('**/*.parquet'))\n",
    "\n",
    "    if max_flights:\n",
    "        parquet_files = parquet_files[:max_flights]\n",
    "\n",
    "    for pq_file in tqdm(parquet_files, desc=\"Loading flights\"):\n",
    "        try:\n",
    "            df = pl.read_parquet(pq_file)\n",
    "\n",
    "            # Check if all signals exist\n",
    "            missing = [s for s in signals if s not in df.columns]\n",
    "            if missing:\n",
    "                continue\n",
    "\n",
    "            # Extract signals and drop NaNs\n",
    "            data = df.select(signals).to_numpy()\n",
    "\n",
    "            # Skip if too many NaNs\n",
    "            nan_pct = np.isnan(data).sum() / data.size\n",
    "            if nan_pct > 0.1:\n",
    "                continue\n",
    "\n",
    "            # Forward fill then backward fill remaining NaNs\n",
    "            for i in range(data.shape[1]):\n",
    "                mask = np.isnan(data[:, i])\n",
    "                if mask.any():\n",
    "                    # Forward fill\n",
    "                    idx = np.where(~mask, np.arange(len(mask)), 0)\n",
    "                    np.maximum.accumulate(idx, out=idx)\n",
    "                    data[:, i] = data[idx, i]\n",
    "\n",
    "            flights.append({\n",
    "                'data': data.astype(np.float32),\n",
    "                'file': pq_file.name,\n",
    "                'tail': pq_file.parent.name.replace('tail_number=', ''),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {pq_file}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(flights)} flights\")\n",
    "    return flights\n",
    "\n",
    "# Load data (start with subset for testing)\n",
    "flights = load_flight_data(CONFIG['data_dir'], ALL_SIGNALS, max_flights=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b31066",
   "metadata": {},
   "source": [
    "# 3: Create Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(flights: list, window_size: int, stride: int) -> np.ndarray:\n",
    "    \"\"\"Create overlapping sequences from flight data.\"\"\"\n",
    "    sequences = []\n",
    "    sequence_metadata = []\n",
    "\n",
    "    for flight in tqdm(flights, desc=\"Creating sequences\"):\n",
    "        data = flight['data']\n",
    "        n_samples = len(data)\n",
    "\n",
    "        for start in range(0, n_samples - window_size + 1, stride):\n",
    "            seq = data[start:start + window_size]\n",
    "            sequences.append(seq)\n",
    "            sequence_metadata.append({\n",
    "                'file': flight['file'],\n",
    "                'tail': flight['tail'],\n",
    "                'start_idx': start,\n",
    "            })\n",
    "\n",
    "    sequences = np.array(sequences)\n",
    "    print(f\"Created {len(sequences)} sequences, shape: {sequences.shape}\")\n",
    "    return sequences, sequence_metadata\n",
    "\n",
    "sequences, seq_metadata = create_sequences(\n",
    "    flights,\n",
    "    CONFIG['window_size'],\n",
    "    CONFIG['stride']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db45d01e",
   "metadata": {},
   "source": [
    "\n",
    "# 4: Normalize and Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit scaler on training data only\n",
    "n_train = int(len(sequences) * CONFIG['train_split'])\n",
    "train_sequences = sequences[:n_train]\n",
    "test_sequences = sequences[n_train:]\n",
    "\n",
    "print(f\"Train: {len(train_sequences)}, Test: {len(test_sequences)}\")\n",
    "\n",
    "# Fit scaler on training data (flatten to 2D for fitting)\n",
    "scaler = StandardScaler()\n",
    "train_flat = train_sequences.reshape(-1, train_sequences.shape[-1])\n",
    "scaler.fit(train_flat)\n",
    "\n",
    "# Transform both sets\n",
    "def normalize_sequences(seqs, scaler):\n",
    "    \"\"\"Normalize sequences using fitted scaler.\"\"\"\n",
    "    original_shape = seqs.shape\n",
    "    flat = seqs.reshape(-1, seqs.shape[-1])\n",
    "    normalized = scaler.transform(flat)\n",
    "    return normalized.reshape(original_shape).astype(np.float32)\n",
    "\n",
    "train_normalized = normalize_sequences(train_sequences, scaler)\n",
    "test_normalized = normalize_sequences(test_sequences, scaler)\n",
    "\n",
    "print(f\"Train mean: {train_normalized.mean():.4f}, std: {train_normalized.std():.4f}\")\n",
    "print(f\"Test mean: {test_normalized.mean():.4f}, std: {test_normalized.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52de28b",
   "metadata": {},
   "source": [
    "\n",
    "# 5: PyTorch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ca954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FlightSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences: np.ndarray):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "train_dataset = FlightSequenceDataset(train_normalized)\n",
    "test_dataset = FlightSequenceDataset(test_normalized)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a3374",
   "metadata": {},
   "source": [
    "\n",
    "# 6: Transformer Autoencoder Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        d_model: int = 64,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        window_size: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(n_features, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=window_size)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(d_model, n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, n_features)\n",
    "\n",
    "        # Encode\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        # Decode (using encoded as memory)\n",
    "        decoded = self.decoder(x, encoded)\n",
    "\n",
    "        # Project back to feature space\n",
    "        output = self.output_proj(decoded)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerAutoencoder(\n",
    "    n_features=len(ALL_SIGNALS),\n",
    "    d_model=CONFIG['latent_dim'],\n",
    "    n_heads=CONFIG['n_heads'],\n",
    "    n_layers=CONFIG['n_layers'],\n",
    "    dropout=CONFIG['dropout'],\n",
    "    window_size=CONFIG['window_size'],\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5523ceec",
   "metadata": {},
   "source": [
    "\n",
    "# 7: Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584147d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs']}: Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eead33",
   "metadata": {},
   "source": [
    "\n",
    "# 8: Plot Training Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(train_losses, label='Train Loss')\n",
    "ax.plot(test_losses, label='Test Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Training Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72036d28",
   "metadata": {},
   "source": [
    "\n",
    "# 9: Compute Anomaly Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_anomaly_scores(model, loader, device):\n",
    "    \"\"\"Compute reconstruction error for each sequence.\"\"\"\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            output = model(batch)\n",
    "\n",
    "            # MSE per sequence (mean over time and features)\n",
    "            mse = ((output - batch) ** 2).mean(dim=(1, 2))\n",
    "            scores.extend(mse.cpu().numpy())\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "train_scores = compute_anomaly_scores(model, train_loader, device)\n",
    "test_scores = compute_anomaly_scores(model, test_loader, device)\n",
    "\n",
    "print(f\"Train scores - Mean: {train_scores.mean():.6f}, Std: {train_scores.std():.6f}\")\n",
    "print(f\"Test scores - Mean: {test_scores.mean():.6f}, Std: {test_scores.std():.6f}\")\n",
    "\n",
    "# Set threshold (e.g., mean + 3*std from training)\n",
    "threshold = train_scores.mean() + 3 * train_scores.std()\n",
    "print(f\"Anomaly threshold: {threshold:.6f}\")\n",
    "\n",
    "# Flag anomalies\n",
    "train_anomalies = train_scores > threshold\n",
    "test_anomalies = test_scores > threshold\n",
    "print(f\"Train anomalies: {train_anomalies.sum()} ({train_anomalies.mean()*100:.2f}%)\")\n",
    "print(f\"Test anomalies: {test_anomalies.sum()} ({test_anomalies.mean()*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d13f05",
   "metadata": {},
   "source": [
    "\n",
    "# 10: Visualize Anomaly Score Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ea465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Histogram\n",
    "ax1 = axes[0]\n",
    "ax1.hist(train_scores, bins=50, alpha=0.7, label='Train', density=True)\n",
    "ax1.hist(test_scores, bins=50, alpha=0.7, label='Test', density=True)\n",
    "ax1.axvline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "ax1.set_xlabel('Reconstruction Error (MSE)')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Anomaly Score Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Top anomalies\n",
    "ax2 = axes[1]\n",
    "top_k = 50\n",
    "top_indices = np.argsort(test_scores)[-top_k:][::-1]\n",
    "ax2.bar(range(top_k), test_scores[top_indices])\n",
    "ax2.axhline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "ax2.set_xlabel('Rank')\n",
    "ax2.set_ylabel('Anomaly Score')\n",
    "ax2.set_title(f'Top {top_k} Anomalous Sequences')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcab32",
   "metadata": {},
   "source": [
    "\n",
    "# 11: Inspect Top Anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6000e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_anomaly(model, sequence, scaler, signals, device):\n",
    "    \"\"\"Plot original vs reconstructed for an anomalous sequence.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        seq_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)\n",
    "        reconstructed = model(seq_tensor).cpu().numpy()[0]\n",
    "\n",
    "    # Denormalize for interpretability\n",
    "    original_denorm = scaler.inverse_transform(sequence)\n",
    "    recon_denorm = scaler.inverse_transform(reconstructed)\n",
    "\n",
    "    n_signals = len(signals)\n",
    "    fig, axes = plt.subplots(n_signals, 1, figsize=(14, 2*n_signals), sharex=True)\n",
    "\n",
    "    time = np.arange(len(sequence)) / 4  # Convert to seconds\n",
    "\n",
    "    for i, (ax, signal) in enumerate(zip(axes, signals)):\n",
    "        ax.plot(time, original_denorm[:, i], label='Original', alpha=0.8)\n",
    "        ax.plot(time, recon_denorm[:, i], label='Reconstructed', alpha=0.8)\n",
    "        ax.set_ylabel(signal)\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[-1].set_xlabel('Time (seconds)')\n",
    "    axes[0].set_title('Anomalous Sequence: Original vs Reconstructed')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot top anomaly\n",
    "top_anomaly_idx = np.argmax(test_scores)\n",
    "print(f\"Top anomaly - Score: {test_scores[top_anomaly_idx]:.6f}\")\n",
    "print(f\"Metadata: {seq_metadata[n_train + top_anomaly_idx]}\")\n",
    "\n",
    "plot_anomaly(model, test_normalized[top_anomaly_idx], scaler, ALL_SIGNALS, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301dde20",
   "metadata": {},
   "source": [
    "\n",
    "# 12: Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model and scaler\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'signals': ALL_SIGNALS,\n",
    "    'threshold': threshold,\n",
    "    'train_scores_mean': train_scores.mean(),\n",
    "    'train_scores_std': train_scores.std(),\n",
    "}, '../models/engine_anomaly_detector.pt')\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/engine_scaler.pkl')\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
